import analysis.PCAanalysis.directParams as DP
import tools.dfTools as DT
import analysis.PCA as PCA

# æ ¹æ®åç§°ä¸€ä¸ªä¸ªè·å–å¯¹åº”å‚æ•°
def getFeatures(features,stockdata):
    return DP.get_multiple_columns(stockdata,features)

# æ ¹æ®ä¼ å…¥çš„æ•°æ®è·å–PCAåˆ†æç»“æœ
def PCAResult(featuresName,stockdata):
    # å°†æ¶¨è·Œå¹…æ·»åŠ åˆ°æœ€åä¸€åˆ—
    reDF = DT.reshape_stock_data(stockdata)
    # æ·»åŠ æœ€åä¸€åˆ—
    addFeatures = featuresName + ["æ˜æ—¥æ¶¨è·Œå¹…"]
    # é¦–å…ˆæ·»åŠ è¦åˆ†æçš„åˆ—
    addDF = getFeatures(addFeatures,reDF)
    # è¿›è¡Œåˆ†æ
    # ç”Ÿæˆæ•°æ®
    features = DT.extract_columns_to_ndarray(addDF,featuresName)
    target = DT.extract_columns_to_ndarray(addDF,"æ˜æ—¥æ¶¨è·Œå¹…")
    # åˆ›å»ºç›¸ä¼¼åº¦åˆ†æå™¨
    similarity_analyzer = PCA.PCASimilarity(n_components=3, feature_names=featuresName)

    # æ‹Ÿåˆæ¨¡å‹
    similarity_analyzer.fit(features, target)

    # è·å–PCAæ‘˜è¦
    summary = similarity_analyzer.get_pca_summary()

    print("PCAæ¨¡å‹æ‘˜è¦:")
    print(f"ä¸»æˆåˆ†æ•°é‡: {summary['n_components']}")
    print(f"æ–¹å·®è§£é‡Šæ¯”ä¾‹: {summary['explained_variance_ratio']}")
    print(f"ç´¯è®¡æ–¹å·®è§£é‡Š: {summary['cumulative_variance_ratio']}")

    # å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ’åº
    rankings = similarity_analyzer.rank_features()
    print(f"\nç‰¹å¾ç›¸ä¼¼åº¦æ’å:")
    for i, (feature_name, score) in enumerate(rankings):
        print(f"{i + 1}. {feature_name}: {score:.4f}")

    print("=" * 50)
    print("ğŸ“ˆ PCAåˆ†æè¯¦ç»†æŠ¥å‘Š")
    print("=" * 50)

    print("ğŸ” æ¨¡å‹åŸºæœ¬ä¿¡æ¯:")
    print(f"   ä¸»æˆåˆ†æ•°é‡: {summary['n_components']}")
    print(f"   åŸå§‹ç‰¹å¾æ•°: {len(featuresName)}")
    print(f"   é™ç»´æ¯”ä¾‹: {summary['n_components']}/{len(featuresName)}")

    print("\nğŸ“Š æ–¹å·®è§£é‡Šåˆ†æ:")
    # ä½¿ç”¨ç‰¹å¾åç§°è€Œä¸æ˜¯æ•°å­—ç¼–å·
    for i, (variance, cumulative) in enumerate(zip(
            summary['explained_variance_ratio'],
            summary['cumulative_variance_ratio']
    )):
        print(f"   ç»´åº¦ {i + 1}: {variance:.1%} (ç´¯è®¡: {cumulative:.1%})")

    # æ–°å¢ï¼šæ˜¾ç¤ºæ¯ä¸ªä¸»æˆåˆ†çš„ä¸»è¦è´¡çŒ®ç‰¹å¾
    print("\nğŸ”¬ ä¸»æˆåˆ†ç‰¹å¾è´¡çŒ®åˆ†æ:")
    components = similarity_analyzer.components_
    feature_names = featuresName + ["æ˜æ—¥æ¶¨è·Œå¹…"]  # åŒ…å«ç›®æ ‡å˜é‡

    for i in range(summary['n_components']):
        print(f"\n   ä¸»æˆåˆ† {i + 1} (è§£é‡Šæ–¹å·®: {summary['explained_variance_ratio'][i]:.1%}):")

        # è·å–å½“å‰ä¸»æˆåˆ†çš„è½½è·å‘é‡
        loadings = components[i]

        # åˆ›å»ºç‰¹å¾-è½½è·å¯¹çš„åˆ—è¡¨å¹¶æ’åº
        feature_loadings = []
        for j, feature_name in enumerate(feature_names):
            feature_loadings.append((feature_name, loadings[j]))

        # æŒ‰è½½è·ç»å¯¹å€¼é™åºæ’åº
        feature_loadings.sort(key=lambda x: abs(x[1]), reverse=True)

        # æ˜¾ç¤ºå‰3ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        for k in range(min(3, len(feature_loadings))):
            feature, loading = feature_loadings[k]
            direction = "æ­£å‘" if loading > 0 else "è´Ÿå‘"
            print(f"     {k + 1}. {feature}: {loading:.3f} ({direction}è´¡çŒ®)")

    print("\nğŸ’¡ ç»“æœè§£è¯»:")
    total_variance = summary['cumulative_variance_ratio'][-1]
    if total_variance > 0.8:
        print(f"   âœ… ä¼˜ç§€: å‰{summary['n_components']}ä¸ªä¸»æˆåˆ†ä¿ç•™äº†{total_variance:.1%}çš„ä¿¡æ¯")
        print("   å»ºè®®: å½“å‰ä¸»æˆåˆ†æ•°é‡è®¾ç½®åˆç†")
    elif total_variance > 0.7:
        print(f"   âš ï¸ è‰¯å¥½: ä¿ç•™äº†{total_variance:.1%}çš„ä¿¡æ¯ï¼Œå¯è€ƒè™‘å¢åŠ ä¸»æˆåˆ†")
    else:
        print(f"   âŒ ä¸è¶³: ä»…ä¿ç•™{total_variance:.1%}çš„ä¿¡æ¯")
        print("   å»ºè®®: å¢åŠ ä¸»æˆåˆ†æ•°é‡æˆ–æ£€æŸ¥æ•°æ®è´¨é‡")

    print("\nğŸ¯ æŠ•èµ„æ´å¯Ÿ:")
    # åŸºäºä¸»æˆåˆ†åˆ†ææä¾›æŠ•èµ„å»ºè®®
    if len(rankings) > 0:
        top_feature = rankings[0][0]  # æ’åç¬¬ä¸€çš„ç‰¹å¾
        print(f"   æœ€ç›¸å…³ç‰¹å¾: '{top_feature}' ä¸æ˜æ—¥æ¶¨è·Œå¹…å…³è”åº¦æœ€é«˜")
        print("   å»ºè®®é‡ç‚¹å…³æ³¨è¯¥æŒ‡æ ‡çš„èµ°åŠ¿å˜åŒ–")

    print("=" * 50)

    return similarity_analyzer